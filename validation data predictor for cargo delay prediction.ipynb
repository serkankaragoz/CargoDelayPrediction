{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae6e240a",
   "metadata": {},
   "source": [
    "### OCI Data Science - Useful Tips\n",
    "<details>\n",
    "<summary><font size=\"2\">Check for Public Internet Access</font></summary>\n",
    "\n",
    "```python\n",
    "import requests\n",
    "response = requests.get(\"https://oracle.com\")\n",
    "assert response.status_code==200, \"Internet connection failed\"\n",
    "```\n",
    "</details>\n",
    "<details>\n",
    "<summary><font size=\"2\">Helpful Documentation </font></summary>\n",
    "<ul><li><a href=\"https://docs.cloud.oracle.com/en-us/iaas/data-science/using/data-science.htm\">Data Science Service Documentation</a></li>\n",
    "<li><a href=\"https://docs.cloud.oracle.com/iaas/tools/ads-sdk/latest/index.html\">ADS documentation</a></li>\n",
    "</ul>\n",
    "</details>\n",
    "<details>\n",
    "<summary><font size=\"2\">Typical Cell Imports and Settings for ADS</font></summary>\n",
    "\n",
    "```python\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.ERROR)\n",
    "\n",
    "import ads\n",
    "from ads.dataset.factory import DatasetFactory\n",
    "from ads.automl.provider import OracleAutoMLProvider\n",
    "from ads.automl.driver import AutoML\n",
    "from ads.evaluations.evaluator import ADSEvaluator\n",
    "from ads.common.data import ADSData\n",
    "from ads.explanations.explainer import ADSExplainer\n",
    "from ads.explanations.mlx_global_explainer import MLXGlobalExplainer\n",
    "from ads.explanations.mlx_local_explainer import MLXLocalExplainer\n",
    "from ads.catalog.model import ModelCatalog\n",
    "from ads.common.model_artifact import ModelArtifact\n",
    "```\n",
    "</details>\n",
    "<details>\n",
    "<summary><font size=\"2\">Useful Environment Variables</font></summary>\n",
    "\n",
    "```python\n",
    "import os\n",
    "print(os.environ[\"NB_SESSION_COMPARTMENT_OCID\"])\n",
    "print(os.environ[\"PROJECT_OCID\"])\n",
    "print(os.environ[\"USER_OCID\"])\n",
    "print(os.environ[\"TENANCY_OCID\"])\n",
    "print(os.environ[\"NB_REGION\"])\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9380e0f2",
   "metadata": {},
   "source": [
    "## This notebook retrieves data on csv format and produces a new csv file with predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8792ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"CARGO_FOTP_DATASET_NEW.csv\" # file path will be used on machinelearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1f0d2d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation data path that contains data to predict\n",
    "validation_data_path = \"CARGO_FOTP_TEAMNAME.csv\"\n",
    "\n",
    "predicted_validation_data_output_path = \"FOTP_HAMSI_3.csv\" # final path that will contain predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c8ce4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pandas import isna\n",
    "\n",
    "\n",
    "import datetime\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d123a4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "\n",
    "class invalid_date_exception(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "class date():\n",
    "    \n",
    "    # {1:31, 2:28, 3:31, 4:30, 5:31, 6:30, 7:31, 8:31, 9:30, 10:31, 11:30, 12:31}\n",
    "    # The object that specified above and used multiple time on date class, enables us to store length of months. \n",
    "    \n",
    "    \n",
    "    # returns True if the date object has valid date values.\n",
    "    def is_date_valid(self):\n",
    "        \n",
    "        \n",
    "        \n",
    "        if(self.month > 12):\n",
    "            return False\n",
    "        elif(self.month == 2 and self.year % 4 == 0 and self.day == 29):\n",
    "            return True\n",
    "        elif(self.day > {1:31, 2:28, 3:31, 4:30, 5:31, 6:30, 7:31, 8:31, 9:30, 10:31, 11:30, 12:31}[self.month]): \n",
    "            return False\n",
    "        return True    \n",
    "\n",
    "    # date variable is an integer which writeen as YYYYMMDD\n",
    "    # time variable is an integer which writeen as HHMM\n",
    "\n",
    "    def __init__(self, date, time):\n",
    "        \n",
    "        #try:\n",
    "            \n",
    "        date = str(date)\n",
    "        dl = len(date) # length of date variable\n",
    "\n",
    "        dl -= 4 \n",
    "\n",
    "        self.year = int(date[0:dl])\n",
    "        self.month = int(date[dl: dl + 2])\n",
    "        self.day = int(date[dl + 2:dl + 4])\n",
    "        self.date = int(date)\n",
    "\n",
    "        if(not self.is_date_valid()):\n",
    "            raise invalid_date_exception(\"ERROR: Invalid date entry detected\")\n",
    "        \n",
    "        \n",
    "        time = str(time)\n",
    "        tl = len(time) # length of time variable\n",
    "        \n",
    "        if(tl <= 2):\n",
    "            self.hour = 0\n",
    "            self.minute = int(time)\n",
    "        else:\n",
    "            self.hour = int(time[0:tl-2])\n",
    "            self.minute = int(time[tl-2:tl])\n",
    "        \n",
    "        \n",
    "    def print_date(self):\n",
    "        print(\"Year:\" + str(self.year) + \", Month:\" + str(self.month) + \", Day:\" + str(self.day))\n",
    "    \n",
    "    def get_date(self):\n",
    "        return self.date\n",
    "    \n",
    "    def get_year(self):\n",
    "        return self.year\n",
    "    \n",
    "    # updates the variables of date object with the date that one day later.\n",
    "    def add_one_day(self):\n",
    "        \n",
    "        #month_length = {1:31, 2:28, 3:31, 4:30, 5:31, 6:30, 7:31, 8:31, 9:30, 10:31, 11:30, 12:31}\n",
    "        \n",
    "        if(self.is_date_valid() is False):\n",
    "            raise invalid_date_exception(\"ERROR: Invalid date entry detected\")\n",
    "        \n",
    "        if(self.day < {1:31, 2:28, 3:31, 4:30, 5:31, 6:30, 7:31, 8:31, 9:30, 10:31, 11:30, 12:31}[self.month]):\n",
    "            self.day += 1\n",
    "        elif(self.month == 2 and self.year % 4 == 0 and self.day < 29):\n",
    "            #self.day = 1\n",
    "            self.day += 1\n",
    "        elif(self.month < 12):\n",
    "            self.day = 1\n",
    "            self.month += 1\n",
    "        else:\n",
    "            self.day = 1\n",
    "            self.month = 1\n",
    "            self.year += 1\n",
    "            \n",
    "        month_str = str(self.month)\n",
    "        if(len(month_str) == 1):\n",
    "            month_str = \"0\" + month_str\n",
    "            \n",
    "        day_str = str(self.day)\n",
    "        if(len(day_str) == 1):\n",
    "            day_str = \"0\" + day_str\n",
    "            \n",
    "        self.date = int(str(self.year) + month_str + day_str)\n",
    "        \n",
    "\n",
    "    # returns the elapsed day between two date\n",
    "    def get_day_elapsed(date_start, date_end):\n",
    "        day_elapsed = 0\n",
    "        coeff = 1 # if date_start is later than date_end, this value will be -1\n",
    "        \n",
    "        if(date_start.date > date_end.date):\n",
    "            date_temp = date_start\n",
    "            date_start = date_end\n",
    "            date_end = date_temp        \n",
    "            coeff = -1\n",
    "            \n",
    "        while(date_start.date != date_end.date):\n",
    "            date_start.add_one_day()\n",
    "            day_elapsed += 1\n",
    "        \n",
    "        return day_elapsed * coeff\n",
    "            \n",
    "        \n",
    "        \n",
    "    # returns the elapsed minute between two date\n",
    "    def get_minute_elapsed(date_start, date_end):\n",
    "        day_elapsed = date.get_day_elapsed(date_start, date_end)\n",
    "        \n",
    "        hour_elapsed = date_end.hour - date_start.hour\n",
    "        minute_elapsed = date_end.minute - date_start.minute\n",
    "        \n",
    "        return day_elapsed * 60 * 24 + hour_elapsed * 60 + minute_elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39c90498",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/home/datascience/spark_conf_dir/oci-hdfs-full-3.3.0.6.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/datascience/conda/pyspark30_p37_cpu_v2/lib/python3.7/site-packages/pyspark/jars/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Kargo tehir tahminleme deneme\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a7c5138",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dda51953",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.format('com.databricks.spark.csv') \\\n",
    "            .options(header='true', inferschema='true') \\\n",
    "            .load(path, header=True);\n",
    "\n",
    "validationData = spark.read.format('com.databricks.spark.csv') \\\n",
    "            .options(header='true', inferschema='true') \\\n",
    "            .load(validation_data_path, header=True);\n",
    "\n",
    "validationData = validationData.withColumn(\"DELAY_FLAG\",lit(\"NONE\"))\n",
    "\n",
    "\n",
    "val_data_cols = validationData.columns\n",
    "val_data_cols.append(\"IS_DELAYED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66a9dfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function checks whether delay occurs or not with reading TOT_DELAY_DURATION column\n",
    "def is_delayed(x):\n",
    "    if int(x) > 0:\n",
    "        return \"DELAYED\"\n",
    "    else: \n",
    "        return \"NOT_DELAYED\"\n",
    "\n",
    "from pyspark.sql.functions import udf, lit\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "\n",
    "is_delayed_udf = udf(lambda x: is_delayed(x), StringType())\n",
    "df = df.withColumn(\"IS_DELAYED\", is_delayed_udf(\"TOT_DELAY_DURATION\"))\n",
    "df = df.withColumn(\"ID\",lit(\"NONE\"))\n",
    "df = df.withColumn(\"DELAY_FLAG\", lit(\"NONE\"))\n",
    "\n",
    "df = df.select(val_data_cols)\n",
    "\n",
    "validationData = validationData.withColumn(\"IS_DELAYED\", lit(\"?\"))\n",
    "\n",
    "df = df.union(validationData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ee0ac10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IS_DELAYED</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DELAYED</td>\n",
       "      <td>63399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NOT_DELAYED</td>\n",
       "      <td>36147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>?</td>\n",
       "      <td>5392</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    IS_DELAYED  count\n",
       "0      DELAYED  63399\n",
       "1  NOT_DELAYED  36147\n",
       "2            ?   5392"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupBy(\"IS_DELAYED\").count().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a015f19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio: 2\n"
     ]
    }
   ],
   "source": [
    "#Multiplying delay occured flights by 2.\n",
    "\n",
    "from pyspark.sql.functions import col, explode, array, lit\n",
    "\n",
    "major_df = df.filter(col(\"IS_DELAYED\") == \"DELAYED\")\n",
    "minor_df = df.filter(col(\"IS_DELAYED\") == \"NOT_DELAYED\")\n",
    "other_df = df.filter(col(\"IS_DELAYED\") == \"?\")\n",
    "ratio = 2\n",
    "print(\"ratio: {}\".format(ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "985e4545",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = range(ratio)\n",
    "oversampled_df = minor_df.withColumn(\"dummy\", explode(array([lit(x) for x in a]))).drop('dummy')\n",
    "\n",
    "# combining delay occured flights with copied rows\n",
    "combined_df = major_df.unionAll(oversampled_df)\n",
    "combined_df = combined_df.unionAll(other_df)\n",
    "df = combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddeebe1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IS_DELAYED</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DELAYED</td>\n",
       "      <td>63399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NOT_DELAYED</td>\n",
       "      <td>72294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>?</td>\n",
       "      <td>5392</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    IS_DELAYED  count\n",
       "0      DELAYED  63399\n",
       "1  NOT_DELAYED  72294\n",
       "2            ?   5392"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupBy(\"IS_DELAYED\").count().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e881d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions that returns dates written as YYYYMMDD to year, month, or day\n",
    "\n",
    "def date_to_year(x):\n",
    "    return int(str(x)[0:4])\n",
    "\n",
    "def date_to_month(x):\n",
    "    return int(str(x)[4:6])\n",
    "\n",
    "def date_to_day(x):\n",
    "    return int(str(x)[6:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7281782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions that returns times written as HHMM to hour or minute\n",
    "def time_to_hour(x):\n",
    "    \n",
    "    d = len(str(x)) # amount of digit\n",
    "    x = int(x)\n",
    "    \n",
    "    if(d <= 2):\n",
    "        return 0\n",
    "    return int(str(x)[0:d-2]) \n",
    "    \n",
    "def time_to_minute(x):\n",
    "    \n",
    "    d = len(str(x)) # amount of digit\n",
    "    x = int(x)\n",
    "    \n",
    "    if(d <= 2):\n",
    "        return x   \n",
    "    return int(str(x)[d-2:d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0d1ed20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns minute elapsed between two dates\n",
    "def get_flight_time(date1, time1, date2, time2): \n",
    "    return date.get_minute_elapsed(date(date1, time1), date(date2, time2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd0e2e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_flight_time_udf = udf(lambda d1, t1, d2, t2: get_flight_time(d1, t1, d2, t2), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86a62b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"MINUTE_ELAPSED\", get_flight_time_udf(\"LEG_DEP_GMT_DT\", \"LEG_DEP_GMT_TM\", \"LEG_ARR_GMT_DT\", \"LEG_ARR_GMT_TM\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91d901e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_to_year_udf = udf(lambda x: date_to_year(x), IntegerType())\n",
    "date_to_month_udf = udf(lambda x: date_to_month(x), IntegerType())\n",
    "date_to_day_udf = udf(lambda x: date_to_day(x), IntegerType())\n",
    "time_to_hour_udf = udf(lambda x: time_to_hour(x), IntegerType())\n",
    "time_to_minute_udf = udf(lambda x: time_to_minute(x), IntegerType())\n",
    "\n",
    "length_udf = udf(lambda x: len(str(x)), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5540edd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "15ccd6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f5a4ba38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# producing new columns with data obtained LEG_DEP_GMT_DT and LEG_DEP_GMT_TM column \n",
    "df = df.withColumn(\"LEG_DEP_GMT_YEAR\", date_to_year_udf(\"LEG_DEP_GMT_DT\"))\n",
    "df = df.withColumn(\"LEG_DEP_GMT_MONTH\", date_to_month_udf(\"LEG_DEP_GMT_DT\"))\n",
    "df = df.withColumn(\"LEG_DEP_GMT_DAY\", date_to_day_udf(\"LEG_DEP_GMT_DT\"))\n",
    "\n",
    "df = df.withColumn(\"LEG_DEP_GMT_HOUR\", time_to_hour_udf(\"LEG_DEP_GMT_TM\"))\n",
    "df = df.withColumn(\"LEG_DEP_GMT_MINUTE\", time_to_minute_udf(\"LEG_DEP_GMT_TM\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f51f6e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# producing new columns with data obtained LEG_ARR_GMT_DT and LEG_ARR_GMT_TM column \n",
    "df = df.withColumn(\"LEG_ARR_GMT_YEAR\", date_to_year_udf(\"LEG_ARR_GMT_DT\"))\n",
    "df = df.withColumn(\"LEG_ARR_GMT_MONTH\", date_to_month_udf(\"LEG_ARR_GMT_DT\"))\n",
    "df = df.withColumn(\"LEG_ARR_GMT_DAY\", date_to_day_udf(\"LEG_ARR_GMT_DT\"))\n",
    "\n",
    "df = df.withColumn(\"LEG_ARR_GMT_HOUR\", time_to_hour_udf(\"LEG_ARR_GMT_TM\"))\n",
    "df = df.withColumn(\"LEG_ARR_GMT_MINUTE\", time_to_minute_udf(\"LEG_ARR_GMT_TM\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5b38c398",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.select([\"LEG_DEP_GMT_DT\", \"LEG_DEP_GMT_TM\", \"LEG_ARR_GMT_DT\", \"LEG_ARR_GMT_TM\",\"MINUTE_ELAPSED\"]).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4032aa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"LEG_ARR_GMT_DT_LENGTH\", length_udf(\"LEG_ARR_GMT_DT\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4d9b7d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(df[\"LEG_ARR_GMT_DT_LENGTH\"] == 8) \n",
    "# removing the row on which has 8 digit number on LEG_ARR_GMT_DT column that causes error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ade20242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function which converts the columns hasn't digital values to a shape that StringIndexer and VectorIndexer can understand\n",
    "def get_dummy(df,categoricalCols,continuousCols,labelCol):\n",
    "    from pyspark.ml import Pipeline\n",
    "    from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "    from pyspark.sql.functions import col\n",
    "    indexers = [ StringIndexer(inputCol=c, outputCol=\"{0}_indexed\".format(c)) for c in categoricalCols ]\n",
    "    # default setting: dropLast=True\n",
    "    encoders = [ OneHotEncoder(inputCol=indexer.getOutputCol(),outputCol=\"{0}_encoded\".format(indexer.getOutputCol())) for indexer in indexers ]\n",
    "    assembler = VectorAssembler(inputCols=[encoder.getOutputCol() for encoder in encoders] + continuousCols, outputCol=\"features\")\n",
    "    pipeline = Pipeline(stages=indexers + encoders + [assembler])\n",
    "    model=pipeline.fit(df)\n",
    "    data = model.transform(df)\n",
    "    data = data.withColumn('label',col(labelCol))\n",
    "    return data.select('features','label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b811bfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#categoricalCols = ['FLEET_FAM','FLEET_FAM_GRP','FLT_TYP','LEG_DEP_AP','LEG_DEP_CNTRY_COD','LEG_DEP_RGN_COD','LEG_ARR_AP','LEG_ARR_CNTRY_COD','LEG_ACT_ARR_RGN_COD']\n",
    "#continuousCols = ['LEG_DEP_GMT_YEAR','LEG_DEP_GMT_MONTH','LEG_DEP_GMT_DAY','LEG_DEP_GMT_HOUR','LEG_DEP_GMT_MINUTE','LEG_ARR_GMT_YEAR','LEG_ARR_GMT_MONTH','LEG_ARR_GMT_DAY','LEG_ARR_GMT_HOUR','LEG_ARR_GMT_MINUTE']\n",
    "#continuousCols = ['LEG_DEP_GMT_YEAR','LEG_DEP_GMT_MONTH','LEG_DEP_GMT_DAY','LEG_DEP_GMT_HOUR','LEG_DEP_GMT_MINUTE','LEG_ARR_GMT_YEAR','LEG_ARR_GMT_HOUR','LEG_ARR_GMT_MINUTE']\n",
    "\n",
    "categoricalCols = ['FLT_TYP','LEG_DEP_CNTRY_COD','LEG_DEP_RGN_COD','LEG_ARR_AP','LEG_ARR_CNTRY_COD']\n",
    "continuousCols = ['LEG_DEP_GMT_YEAR','MINUTE_ELAPSED']\n",
    "labelCol = 'IS_DELAYED'\n",
    "#labelCol = 'DELAY1_SUB_CODE'\n",
    "\n",
    "# columns that are useful for machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0c7a8491",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_features = get_dummy(df,categoricalCols,continuousCols,labelCol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b01ff54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors # !!!! from pyspark.mllib.linalg import Vectors DEĞİL !!!!\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import IndexToString,StringIndexer, VectorIndexer\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "def transData(data):\n",
    "    return data.rdd.map(lambda r: [Vectors.dense(r[:-1]),r[-1]]).toDF(['features','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "050de609",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+------------+\n",
      "|            features|  label|indexedLabel|\n",
      "+--------------------+-------+------------+\n",
      "|(442,[1,113,187,3...|DELAYED|         1.0|\n",
      "|(442,[6,114,161,3...|DELAYED|         1.0|\n",
      "|(442,[6,114,161,3...|DELAYED|         1.0|\n",
      "|(442,[6,114,161,3...|DELAYED|         1.0|\n",
      "|(442,[6,114,161,3...|DELAYED|         1.0|\n",
      "+--------------------+-------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "labelIndexer = StringIndexer(inputCol='label', outputCol='indexedLabel').fit(df_features)\n",
    "#df_features\n",
    "labelIndexer.transform(df_features).show(5, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "997825f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+--------------------+\n",
      "|            features|  label|     indexedFeatures|\n",
      "+--------------------+-------+--------------------+\n",
      "|(442,[1,113,187,3...|DELAYED|(442,[1,113,187,3...|\n",
      "|(442,[6,114,161,3...|DELAYED|(442,[6,114,161,3...|\n",
      "|(442,[6,114,161,3...|DELAYED|(442,[6,114,161,3...|\n",
      "|(442,[6,114,161,3...|DELAYED|(442,[6,114,161,3...|\n",
      "|(442,[6,114,161,3...|DELAYED|(442,[6,114,161,3...|\n",
      "+--------------------+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "featureIndexer =VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(df_features)\n",
    "featureIndexer.transform(df_features).show(5, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "91b533fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6b329534",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = df_features.withColumn(\"orderingCol\", monotonically_increasing_id())\n",
    "\n",
    "df = df.withColumn(\"orderingCol\", monotonically_increasing_id())\n",
    "\n",
    "df_features_cols = df_features.columns\n",
    "df_features_cols.remove('orderingCol')\n",
    "\n",
    "df_cols = df.columns\n",
    "df_cols.remove('orderingCol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bd679b54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['features', 'label']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6aba7ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_merged = df.join(df_features, df[\"orderingCol\"] == df_features[\"orderingCol\"])\n",
    "\n",
    "# splitting data as training and validation data\n",
    "(trainingData, testData) = df_merged.select(df_features_cols).filter(df_merged[\"ID\"] == \"NONE\"), df_merged.select(df_features_cols).filter(df_merged[\"ID\"] != \"NONE\")\n",
    "(trainingDF, testDF) = df_merged.select(df_cols).filter(df_merged[\"ID\"] == \"NONE\"), df_merged.select(df_cols).filter(df_merged[\"ID\"] != \"NONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "865594ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|            features|  label|\n",
      "+--------------------+-------+\n",
      "|(442,[1,113,187,3...|DELAYED|\n",
      "|(442,[6,114,161,3...|DELAYED|\n",
      "|(442,[6,114,161,3...|DELAYED|\n",
      "|(442,[6,114,161,3...|DELAYED|\n",
      "|(442,[6,114,161,3...|DELAYED|\n",
      "|(442,[6,114,161,3...|DELAYED|\n",
      "|(442,[6,114,161,3...|DELAYED|\n",
      "|(442,[1,113,187,3...|DELAYED|\n",
      "|(442,[1,113,187,3...|DELAYED|\n",
      "|(442,[1,113,187,3...|DELAYED|\n",
      "+--------------------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_merged.select(df_features_cols).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ed222a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "logr = LogisticRegression(featuresCol='indexedFeatures', labelCol = 'indexedLabel')\n",
    "# applying logistic regression since the value we need to predict has only two possible result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "996aaf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert indexed labels back to original labels.\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol = \"predictedLabel\",labels=labelIndexer.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "895bb7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain indexers and tree in a Pipeline\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, logr, labelConverter])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b9f04b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Training the model with train data.\n",
    "model = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d11e383f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 306:=============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+\n",
      "|            features|predictedLabel|\n",
      "+--------------------+--------------+\n",
      "|(442,[0,1,113,125...|   NOT_DELAYED|\n",
      "|(442,[1,113,133,3...|   NOT_DELAYED|\n",
      "|(442,[12,116,124,...|   NOT_DELAYED|\n",
      "|(442,[1,113,149,3...|   NOT_DELAYED|\n",
      "|(442,[1,113,149,3...|   NOT_DELAYED|\n",
      "+--------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# tahmin yapılıyor.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "predictions.select(\"features\", \"predictedLabel\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7ced0f",
   "metadata": {},
   "source": [
    "#### Now we hace predicted data. All we need is to write our predicted data to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e5959e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "testDF = testDF.withColumn(\"orderingCol2\", monotonically_increasing_id())\n",
    "\n",
    "predictions = predictions.withColumn(\"orderingCol2\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c1c968de",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_cols.remove(\"IS_DELAYED\")\n",
    "val_data_cols.append(\"orderingCol2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cf444d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2eeab806",
   "metadata": {},
   "outputs": [],
   "source": [
    "finalTest = testDF.select(val_data_cols).join(predictions.select('predictedLabel','orderingCol2'), testDF[\"orderingCol2\"] == predictions[\"orderingCol2\"])\n",
    "\n",
    "\n",
    "\n",
    "finalTest = finalTest.withColumn(\"DELAY_ROOT_ARR_AP\",when(finalTest.predictedLabel == \"DELAYED\", finalTest.LEG_ARR_AP)\n",
    "                                                    .when(finalTest.predictedLabel == \"NOT_DELAYED\", \"?\")) \\\n",
    "                     .withColumn(\"DELAY_ROOT_DEP_AP\",when(finalTest.predictedLabel == \"DELAYED\", finalTest.LEG_DEP_AP)\n",
    "                                                    .when(finalTest.predictedLabel == \"NOT_DELAYED\", \"?\")) \\\n",
    "                     .drop(\"DELAY_FLAG\") \\\n",
    "                     .withColumnRenamed(\"predictedLabel\",\"DELAY_FLAG\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b5cc9911",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_columns = ['ID','FLT_NO','LEG_DEP_AP','LEG_DEP_GMT_DT','LEG_DEP_GMT_TM','LEG_ARR_AP','LEG_ARR_GMT_DT','LEG_ARR_GMT_TM','DELAY_FLAG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a555b8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "finalTest = finalTest.drop(\"orderingCol2\")\n",
    "finalTest = finalTest.select(final_columns)\n",
    "finalTest = finalTest.sort(finalTest.FLT_NO.asc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ed8c4828",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "finalTest.repartition(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(predicted_validation_data_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4c3215",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
